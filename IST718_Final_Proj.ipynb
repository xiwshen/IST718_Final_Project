{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IST718_Final_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF5T9TpLWjD2"
      },
      "source": [
        "%%bash\n",
        "# Do not change or modify this file\n",
        "# Need to install pyspark\n",
        "# if pyspark is already installed, will print a message indicating pyspark already isntalled\n",
        "pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi2Y0kH3WuyL"
      },
      "source": [
        "# import statements\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "MAX_MEMORY = \"12g\"\n",
        "spark = SparkSession \\\n",
        "  .builder \\\n",
        "  .master(\"local[*]\")\\\n",
        "  .config(\"spark.memory.fraction\", 0.8) \\\n",
        "  .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
        "  .config(\"spark.driver.memory\", MAX_MEMORY)\\\n",
        "  .config(\"spark.memory.offHeap.enabled\",'true')\\\n",
        "  .config(\"spark.memory.offHeap.size\",MAX_MEMORY)\\\n",
        "  .getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5y5aE-XWxWs"
      },
      "source": [
        "train_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"train.csv\")\n",
        "train_df.take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXJ7_RHwW0C_"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "train_df = train_df.drop('id')\n",
        "display(train_df.toPandas().head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BtoOH17W4oh"
      },
      "source": [
        "import seaborn as sns\n",
        "train_pd_df = train_df.toPandas()\n",
        "sns.countplot(train_pd_df.Gender)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opUk6RjeXAgn"
      },
      "source": [
        "sns.countplot(train_pd_df.Vehicle_Age)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X26HjBrbXDLN"
      },
      "source": [
        "sns.countplot(train_pd_df.Vehicle_Damage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co-SBE1cXKSt"
      },
      "source": [
        "train_df = train_df.withColumn(\"Gender\", when(train_df.Gender == 'Male', 1).otherwise(0))\n",
        "train_df = train_df.withColumn(\"Vehicle_Damage\", when(train_df.Vehicle_Damage == 'Yes', 1).otherwise(0))\n",
        "train_df = train_df.withColumn(\"Vehicle_Age\", when(train_df.Vehicle_Age == '> 2 Years', 2).when(train_df.Vehicle_Age == '1-2 Year', 1).otherwise(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTkvvte0XNIz"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import StringIndexerModel\n",
        "\n",
        "indexer_1 = StringIndexerModel.from_labels(['Female', 'Male'], inputCol=\"Gender\", outputCol=\"Gender_idx\")\n",
        "indexer_2 = StringIndexerModel.from_labels(['No', 'Yes'], inputCol=\"Vehicle_Damage\", outputCol=\"Vehicle_Damage_idx\")\n",
        "indexer_3 = StringIndexerModel.from_labels(['< 1 Year', '1-2 Year', '> 2 Years'], inputCol=\"Vehicle_Age\", outputCol=\"Vehicle_Age_idx\")\n",
        "\n",
        "feature_engineering_pipe = Pipeline(stages=[indexer_1, indexer_2, indexer_3])\n",
        "temp_df = feature_engineering_pipe.fit(train_df).transform(train_df)\n",
        "columns_to_drop = ['Gender', 'Vehicle_Damage', 'Vehicle_Age']\n",
        "temp_df = temp_df.drop(*columns_to_drop)\n",
        "train_df_new = temp_df.withColumnRenamed(\"Gender_idx\", \"Gender\").withColumnRenamed(\"Vehicle_Damage_idx\", \"Vehicle_Damage\").withColumnRenamed(\"Vehicle_Age_idx\", \"Vehicle_Age\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM3XuCnZXQgt"
      },
      "source": [
        "display(train_df_new.toPandas().head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdOet1ZtXS9r"
      },
      "source": [
        "train_df_new.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK79a6JCXWEU"
      },
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "train_df_new = train_df_new.withColumn(\"Age\", train_df_new[\"Age\"].cast(DoubleType()))\n",
        "train_df_new = train_df_new.withColumn(\"Driving_License\", train_df_new[\"Driving_License\"].cast(DoubleType()))\n",
        "train_df_new = train_df_new.withColumn(\"Region_Code\", train_df_new[\"Region_Code\"].cast(DoubleType()))\n",
        "train_df_new = train_df_new.withColumn(\"Previously_Insured\", train_df_new[\"Previously_Insured\"].cast(DoubleType()))\n",
        "train_df_new = train_df_new.withColumn(\"Annual_Premium\", train_df_new[\"Annual_Premium\"].cast(DoubleType()))\n",
        "train_df_new = train_df_new.withColumn(\"Policy_Sales_Channel\", train_df_new[\"Policy_Sales_Channel\"].cast(DoubleType()))\n",
        "train_df_new = train_df_new.withColumn(\"Vintage\", train_df_new[\"Vintage\"].cast(DoubleType()))\n",
        "train_df_new = train_df_new.withColumn(\"Response\", train_df_new[\"Response\"].cast(DoubleType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgRZ-6f5XYur"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "#train_df.dtypes\n",
        "vecAssembler = VectorAssembler(inputCols=['Gender', 'Age', 'Driving_License', 'Region_Code', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage'], outputCol=\"features\")\n",
        "#train_df = vecAssembler.transform(train_df)\n",
        "sc = StandardScaler(withMean=True, withStd=True, inputCol='features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd_-aoaKXf59"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Response\", metricName=\"areaUnderROC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWq-6KXfXl2a"
      },
      "source": [
        "training_df, testing_df = train_df_new.randomSplit([0.7, 0.3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMryKE1-Xs5e"
      },
      "source": [
        "#RF\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(labelCol=\"Response\", featuresCol=\"features\")\n",
        "\n",
        "rf_pipeline = Pipeline(stages=[vecAssembler, rf]).fit(training_df)\n",
        "rf_model = rf_pipeline.transform(testing_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKY-YdDGX4K_"
      },
      "source": [
        "print(evaluator.evaluate(rf_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma2TeaqWX6NL"
      },
      "source": [
        "feature_importances = rf_pipeline.stages[-1].featureImportances\n",
        "feature_imp_array = feature_importances.toArray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNtlxj5UX8c4"
      },
      "source": [
        "print(feature_imp_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0_xiYUcX-9S"
      },
      "source": [
        "predictors = ['Gender', 'Age', 'Driving_License', 'Region_Code', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage']\n",
        "\n",
        "feats = {} # a dict to hold feature_name: feature_importance\n",
        "for feature, importance in zip(predictors, feature_imp_array):\n",
        "    feats[feature] = importance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoxaZ_gfYl77"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
        "importances.sort_values(by='Gini-importance').plot(kind='bar', rot=90, title='RF feature importance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z5DxkMDYpZt"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#actual = rf_model_pd['Response'].tolist()\n",
        "#pred = rf_model_pd['prediction'].tolist()\n",
        "\n",
        "y_true = rf_model.select(['Response']).collect()\n",
        "y_pred = rf_model.select(['prediction']).collect()\n",
        "\n",
        "#print(classification_report(actual, pred))\n",
        "#print(precision_score(actual, pred))\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVazJJi-Za9B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}